{"modelId": "scales-okn/distilroberta-base-dtlm", "sha": "7b4cc4fa8281b77136d0c094198d7d4974cf55f6", "lastModified": "2021-12-17T07:53:08.000Z", "tags": ["pytorch", "roberta", "fill-mask", "transformers", "autonlp_compatible", "infinity_compatible"], "pipeline_tag": "fill-mask", "siblings": [{"rfilename": ".gitattributes"}, {"rfilename": "config.json"}, {"rfilename": "merges.txt"}, {"rfilename": "optimizer.pt"}, {"rfilename": "pytorch_model.bin"}, {"rfilename": "rng_state.pth"}, {"rfilename": "scheduler.pt"}, {"rfilename": "special_tokens_map.json"}, {"rfilename": "tokenizer.json"}, {"rfilename": "tokenizer_config.json"}, {"rfilename": "trainer_state.json"}, {"rfilename": "training_args.bin"}, {"rfilename": "vocab.json"}], "config": {"architectures": ["RobertaForMaskedLM"], "model_type": "roberta"}, "private": false, "downloads": 2, "library_name": "transformers", "likes": 0}