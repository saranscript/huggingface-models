{"modelId": "deepset/gelectra-large", "sha": "0514524308ddedc8ac121ef5f17a6c17d8ea7786", "lastModified": "2021-10-21T12:19:02.000Z", "tags": ["pytorch", "tf", "electra", "pretraining", "de", "dataset:wikipedia", "dataset:OPUS", "dataset:OpenLegalData", "dataset:oscar", "arxiv:2010.10906", "transformers", "license:mit", "infinity_compatible"], "pipeline_tag": null, "siblings": [{"rfilename": ".gitattributes"}, {"rfilename": "README.md"}, {"rfilename": "config.json"}, {"rfilename": "pytorch_model.bin"}, {"rfilename": "tf_model.h5"}, {"rfilename": "tokenizer_config.json"}, {"rfilename": "vocab.txt"}], "config": {"architectures": ["ElectraForPreTraining"], "model_type": "electra"}, "private": false, "downloads": 1367, "library_name": "transformers", "likes": 5}