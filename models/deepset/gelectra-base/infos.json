{"modelId": "deepset/gelectra-base", "sha": "ea2226bdcf9bb0279f894b263485ceffb62989e3", "lastModified": "2021-10-21T12:18:40.000Z", "tags": ["pytorch", "tf", "electra", "pretraining", "de", "dataset:wikipedia", "dataset:OPUS", "dataset:OpenLegalData", "arxiv:2010.10906", "transformers", "license:mit", "infinity_compatible"], "pipeline_tag": null, "siblings": [{"rfilename": ".gitattributes"}, {"rfilename": "README.md"}, {"rfilename": "config.json"}, {"rfilename": "pytorch_model.bin"}, {"rfilename": "tf_model.h5"}, {"rfilename": "tokenizer_config.json"}, {"rfilename": "vocab.txt"}], "config": {"architectures": ["ElectraForPreTraining"], "model_type": "electra"}, "private": false, "downloads": 1619, "library_name": "transformers", "likes": 3}