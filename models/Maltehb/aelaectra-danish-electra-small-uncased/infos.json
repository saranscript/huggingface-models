{"modelId": "Maltehb/aelaectra-danish-electra-small-uncased", "sha": "687bd788e396966d15da24cba4fc1b64fe9c4c07", "lastModified": "2021-11-23T06:39:20.000Z", "tags": ["pytorch", "electra", "pretraining", "da", "dataset:DAGW", "arxiv:2003.10555", "arxiv:1810.04805", "arxiv:2005.03521", "transformers", "\u00e6l\u00e6ctra", "danish", "ELECTRA-Small", "replaced token detection", "license:mit", "co2_eq_emissions", "infinity_compatible"], "pipeline_tag": null, "siblings": [{"rfilename": ".gitattributes"}, {"rfilename": "README.md"}, {"rfilename": "config.json"}, {"rfilename": "pytorch_model.bin"}, {"rfilename": "special_tokens_map.json"}, {"rfilename": "tokenizer_config.json"}, {"rfilename": "vocab.txt"}], "config": {"architectures": ["ElectraForPreTraining"], "model_type": "electra"}, "private": false, "downloads": 170, "library_name": "transformers", "likes": 0}