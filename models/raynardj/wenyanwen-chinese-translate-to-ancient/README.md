---
language:
- zh
- zh
tags:
- translation
- æ–‡è¨€æ–‡
- ancient
license: apache-2.0
widget:
- text: "è½»è½»çš„æˆ‘èµ°äº†ï¼Œæ­£å¦‚æˆ‘è½»è½»çš„æ¥ã€‚æˆ‘è½»è½»çš„æ‹›æ‰‹ï¼Œä½œåˆ«è¥¿å¤©çš„äº‘å½©ã€‚"
  example_title: "å†åˆ«åº·æ¡¥"
- text: "å½“ææƒ§é€å»ï¼Œæˆ‘ä¼šæ‰“å¼€å¿ƒçœ¼ï¼Œçœ‹æ¸…å®ƒçš„è½¨è¿¹ã€‚"
  example_title: "æ²™ä¸˜"
- text: "æš´åŠ›æ˜¯æ— èƒ½è€…çš„æœ€åæ‰‹æ®µ"
  example_title: "åŸºåœ°"

---

# From modern Chinese to Ancient Chinese
> This model translate modern Chinese to Classical Chinese, so I guess who's interested in the problemset can speak at least modern Chinese, so... let me continue the documentation in Chinese

* ä»ç°ä»£æ–‡åˆ°æ–‡è¨€æ–‡çš„ç¿»è¯‘å™¨, æ¬¢è¿å‰å¾€[githubæ–‡è¨€è¯—è¯é¡¹ç›®é¡µé¢:æ¸Š, è®¨è®º&åŠ â­ï¸ ](https://github.com/raynardj/yuan)

* è¿˜æœ‰åŒæ¬¾çš„[ğŸ¤—æ–‡è¨€æ–‡åˆ°ç°ä»£æ–‡æ¨¡å‹](https://huggingface.co/raynardj/wenyanwen-ancient-translate-to-modern)ï¼ŒåŸæ–‡è¾“å…¥å¯ä»¥**æ–­å¥** ä¹Ÿå¯ä»¥æ˜¯**æœªæ–­å¥**çš„å“¦

* è®­ç»ƒè¯­æ–™æ˜¯å°±æ˜¯ä¹åå¤šä¸‡å¥å¥å¯¹ï¼Œ [æ•°æ®é›†é“¾æ¥ğŸ“š](https://github.com/BangBOOM/Classical-Chinese)ã€‚

## æ¨èçš„inference é€šé“
**æ³¨æ„**ï¼Œ ä½ å¿…é¡»å°†```generate```å‡½æ•°çš„```eos_token_id```è®¾ç½®ä¸º102å°±å¯ä»¥ç¿»è¯‘å‡ºå®Œæ•´çš„è¯­å¥ï¼Œ ä¸ç„¶ç¿»è¯‘å®Œäº†ä¼šæœ‰æ®‹ç•™çš„è¯­å¥(å› ä¸ºåšç†µçš„æ—¶å€™ç”¨padæ ‡ç­¾=-100å¯¼è‡´)ã€‚

ç›®å‰huggingface é¡µé¢ä¸ŠcomputeæŒ‰é’®ä¼šæœ‰è¿™ä¸ªé—®é¢˜ï¼Œ æ¨èä½¿ç”¨ä»¥ä¸‹ä»£ç æ¥å¾—åˆ°ç¿»è¯‘ç»“æœğŸ» 
```python
from transformers import (
  EncoderDecoderModel,
  AutoTokenizer
)
PRETRAINED = "raynardj/wenyanwen-chinese-translate-to-ancient"
tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)
model = EncoderDecoderModel.from_pretrained(PRETRAINED)

def inference(text):
    tk_kwargs = dict(
      truncation=True,
      max_length=128,
      padding="max_length",
      return_tensors='pt')
   
    inputs = tokenizer([text,],**tk_kwargs)
    with torch.no_grad():
        return tokenizer.batch_decode(
            model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,
            num_beams=3,
            bos_token_id=101,
            eos_token_id=tokenizer.sep_token_id,
            pad_token_id=tokenizer.pad_token_id,
        ), skip_special_tokens=True)
```

## ç›®å‰ç‰ˆæœ¬çš„æ¡ˆä¾‹
> å¤§å®¶å¦‚æœæœ‰å¥½ç©çš„è°ƒæˆæ¡ˆä¾‹ï¼Œ ä¹Ÿæ¬¢è¿åé¦ˆ

```python
>>> inference('ä½ è¿ä¸€ç™¾å—éƒ½ä¸è‚¯ç»™æˆ‘')
['ä¸ è‚¯ ä¸ æˆ‘ ç™¾ é’± ã€‚']
```

```python
>>> inference("ä»–ä¸èƒ½åšé•¿è¿œçš„è°‹åˆ’")
['ä¸ èƒ½ ä¸º è¿œ è°‹ ã€‚']
```

```python
>>> inference("æˆ‘ä»¬è¦å¹²ä¸€ç•ªå¤§äº‹ä¸š")
['å¾ å± å½“ ä¸¾ å¤§ äº‹ ã€‚']
```

```python
>>> inference("è¿™æ„Ÿè§‰ï¼Œå·²ç»ä¸å¯¹ï¼Œæˆ‘åŠªåŠ›ï¼Œåœ¨æŒ½å›")
['æ­¤ ä¹‹ è°“ ä¹Ÿ ï¼Œ å·² ä¸ å¯ çŸ£ ï¼Œ æˆ‘ å‹‰ ä¹‹ ï¼Œ ä»¥ å› ä¹‹ ã€‚']
```

```python
>>> inference("è½»è½»åœ°æˆ‘èµ°äº†ï¼Œ æ­£å¦‚æˆ‘è½»è½»åœ°æ¥ï¼Œ æˆ‘æŒ¥ä¸€æŒ¥è¡£è¢–ï¼Œä¸å¸¦èµ°ä¸€ç‰‡äº‘å½©")
['è½» æˆ‘ è¡Œ ï¼Œ å¦‚ æˆ‘ è½» æ¥ ï¼Œ æŒ¥ è¢‚ ä¸ æº ä¸€ ç‰‡ äº‘ ã€‚']
```

## å…¶ä»–æ–‡è¨€è¯—è¯çš„èµ„æº
* [é¡¹ç›®æºä»£ç  ğŸŒŸ, æ¬¢è¿+staræpr](https://github.com/raynardj/yuan)
* [è·¨è¯­ç§æœç´¢ ğŸ”](https://huggingface.co/raynardj/xlsearch-cross-lang-search-zh-vs-classicical-cn)
* [ç°ä»£æ–‡ç¿»è¯‘å¤æ±‰è¯­çš„æ¨¡å‹ â›°](https://huggingface.co/raynardj/wenyanwen-chinese-translate-to-ancient)
* [å¤æ±‰è¯­åˆ°ç°ä»£æ–‡çš„ç¿»è¯‘æ¨¡å‹, è¾“å…¥å¯ä»¥æ˜¯æœªæ–­å¥çš„å¥å­ ğŸš€](https://huggingface.co/raynardj/wenyanwen-ancient-translate-to-modern)
* [æ–­å¥æ¨¡å‹ ğŸ—¡](https://huggingface.co/raynardj/classical-chinese-punctuation-guwen-biaodian)
* [æ„å¢ƒå…³é”®è¯ å’Œ è—å¤´å†™è¯—ğŸ¤–](https://huggingface.co/raynardj/keywords-cangtou-chinese-poetry)
