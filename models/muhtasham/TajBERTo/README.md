---
language: 
  - tg
widget:
- text: "–ü–æ–π—Ç–∞—Ö—Ç–∏ <mask> –î—É—à–∞–Ω–±–µ"
- text: "<mask> –±–∞ –∏–Ω —Å–∞–π—Ç–∏ —à—É–º–æ –º–µ–¥–∞—Ä–æ—è–º."
- text: "–ù–æ–º–∏ –º–∞–Ω –ê–∫—Ä–∞–º <mask>"

tags:
- generated_from_trainer
model_index:
- name: TajBERTo
  results:
  - task:
      name: Masked Language Modeling
      type: fill-mask
---

# TajBERTo: RoBERTa-like Language model trained on Tajik 
## First ever Tajik NLP model üî•



# Dataset:
# This model was trained on filtered and merged version of Leipzig Corpora https://wortschatz.unileipzig.de/en/download/Tajik 

## Intended use 
# You can use the raw model for masked text generation or fine-tune it to a downstream task. 


## Example pipeline
```python
from transformers import pipeline
fill_mask = pipeline(
    "fill-mask",
    model="muhtasham/TajBERTo",
    tokenizer="muhtasham/TajBERTo"
)
fill_mask("–ü–æ–π—Ç–∞—Ö—Ç–∏ <mask> –î—É—à–∞–Ω–±–µ")

# This is the beginning of a beautiful <mask>.

{'score': 0.1952248513698578,
  'sequence': '–ü–æ–π—Ç–∞—Ö—Ç–∏ —à–∞“≥—Ä–∏ –î—É—à–∞–Ω–±–µ',
  'token': 710,
  'token_str': ' —à–∞“≥—Ä–∏'},
 {'score': 0.029092855751514435,
  'sequence': '–ü–æ–π—Ç–∞—Ö—Ç–∏ –¥–∞—Ä –î—É—à–∞–Ω–±–µ',
  'token': 310,
  'token_str': ' –¥–∞—Ä'},
 {'score': 0.020065447315573692,
  'sequence': '–ü–æ–π—Ç–∞—Ö—Ç–∏ –î—É—à–∞–Ω–±–µ –î—É—à–∞–Ω–±–µ',
  'token': 717,
  'token_str': ' –î—É—à–∞–Ω–±–µ'},
 {'score': 0.016725927591323853,
  'sequence': '–ü–æ–π—Ç–∞—Ö—Ç–∏ –¢–æ“∑–∏–∫–∏—Å—Ç–æ–Ω –î—É—à–∞–Ω–±–µ',
  'token': 424,
  'token_str': ' –¢–æ“∑–∏–∫–∏—Å—Ç–æ–Ω'},
 {'score': 0.011400512419641018,
  'sequence': '–ü–æ–π—Ç–∞—Ö—Ç–∏ –∞–∑ –î—É—à–∞–Ω–±–µ',
  'token': 335,
  'token_str': ' –∞–∑'}
  
```


 