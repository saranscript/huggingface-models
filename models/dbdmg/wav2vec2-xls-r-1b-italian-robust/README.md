---
language:
- it
license: apache-2.0
tags:
- automatic-speech-recognition
- generated_from_trainer
- robust-speech-event
datasets:
- mozilla-foundation/common_voice_7_0
model-index:
- name: XLS-R-1b - Italian
  results:
  - task: 
      name: Automatic Speech Recognition 
      type: automatic-speech-recognition
    dataset:
      name: Common Voice 7
      type: mozilla-foundation/common_voice_7_0
      args: it
    metrics:
       - name: Test WER
         type: wer
         value: 32.74
       - name: Test CER
         type: cer
         value: 7.83
       - name: Test WER (+LM)
         type: wer
         value: 19.55
       - name: Test CER (+LM)
         type: cer
         value: 5.59
  - task: 
      name: Automatic Speech Recognition
      type: automatic-speech-recognition
    dataset:
      name: Robust Speech Event - Dev Data
      type: speech-recognition-community-v2/dev_data
      args: it
    metrics:
       - name: Test WER
         type: wer
         value: 43.23
       - name: Test CER
         type: cer
         value: 13.37
       - name: Test WER (+LM)
         type: wer
         value: 27.51
       - name: Test CER (+LM)
         type: cer
         value: 10.69
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# wav2vec2-xls-r-1b-italian-robust

This model is a fine-tuned version of [facebook/wav2vec2-xls-r-1b](https://huggingface.co/facebook/wav2vec2-xls-r-1b) on the Common Voice 7 & Libri Speech datasets.
It achieves the following results on the evaluation set:
- Loss: 0.2428
- Wer: 0.2960

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 32
- eval_batch_size: 16
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 500
- num_epochs: 10.0
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Wer    |
|:-------------:|:-----:|:-----:|:---------------:|:------:|
| No log        | 0.07  | 400   | 1.0053          | 0.8058 |
| 1.5087        | 0.13  | 800   | 0.9127          | 0.8104 |
| 0.9552        | 0.2   | 1200  | 1.0360          | 0.8836 |
| 0.9555        | 0.27  | 1600  | 0.9980          | 0.8577 |
| 1.0259        | 0.34  | 2000  | 1.0103          | 0.8842 |
| 1.0259        | 0.4   | 2400  | 0.9119          | 0.8466 |
| 1.0365        | 0.47  | 2800  | 0.9000          | 0.8281 |
| 1.0069        | 0.54  | 3200  | 0.7976          | 0.7875 |
| 0.9688        | 0.61  | 3600  | 0.8126          | 0.8051 |
| 0.9638        | 0.67  | 4000  | 0.7921          | 0.7903 |
| 0.9638        | 0.74  | 4400  | 0.7703          | 0.7783 |
| 0.9327        | 0.81  | 4800  | 0.7253          | 0.7463 |
| 0.8992        | 0.88  | 5200  | 0.6841          | 0.7171 |
| 0.8693        | 0.94  | 5600  | 0.6867          | 0.7250 |
| 0.8433        | 1.01  | 6000  | 0.7077          | 0.7302 |
| 0.8433        | 1.08  | 6400  | 0.6685          | 0.7091 |
| 0.8499        | 1.14  | 6800  | 0.6355          | 0.6825 |
| 0.8159        | 1.21  | 7200  | 0.6283          | 0.6800 |
| 0.8001        | 1.28  | 7600  | 0.6288          | 0.6743 |
| 0.7883        | 1.35  | 8000  | 0.5995          | 0.6633 |
| 0.7883        | 1.41  | 8400  | 0.6195          | 0.6726 |
| 0.7863        | 1.48  | 8800  | 0.6039          | 0.6588 |
| 0.7713        | 1.55  | 9200  | 0.5842          | 0.6490 |
| 0.7572        | 1.62  | 9600  | 0.5975          | 0.6533 |
| 0.7442        | 1.68  | 10000 | 0.5508          | 0.6233 |
| 0.7442        | 1.75  | 10400 | 0.5521          | 0.6209 |
| 0.7296        | 1.82  | 10800 | 0.5760          | 0.6245 |
| 0.7205        | 1.89  | 11200 | 0.5593          | 0.6144 |
| 0.7106        | 1.95  | 11600 | 0.5672          | 0.6220 |
| 0.7146        | 2.02  | 12000 | 0.5134          | 0.5911 |
| 0.7146        | 2.09  | 12400 | 0.5069          | 0.5811 |
| 0.6944        | 2.15  | 12800 | 0.5022          | 0.5962 |
| 0.6817        | 2.22  | 13200 | 0.4989          | 0.5813 |
| 0.6721        | 2.29  | 13600 | 0.4941          | 0.5742 |
| 0.6774        | 2.36  | 14000 | 0.4775          | 0.5676 |
| 0.6774        | 2.42  | 14400 | 0.4694          | 0.5525 |
| 0.6621        | 2.49  | 14800 | 0.4720          | 0.5514 |
| 0.6599        | 2.56  | 15200 | 0.4714          | 0.5553 |
| 0.6591        | 2.63  | 15600 | 0.4578          | 0.5397 |
| 0.645         | 2.69  | 16000 | 0.4619          | 0.5452 |
| 0.645         | 2.76  | 16400 | 0.4578          | 0.5343 |
| 0.6431        | 2.83  | 16800 | 0.4514          | 0.5328 |
| 0.636         | 2.9   | 17200 | 0.4526          | 0.5325 |
| 0.6433        | 2.96  | 17600 | 0.4561          | 0.5325 |
| 0.6356        | 3.03  | 18000 | 0.4386          | 0.5191 |
| 0.6356        | 3.1   | 18400 | 0.4291          | 0.5065 |
| 0.6175        | 3.16  | 18800 | 0.4306          | 0.5170 |
| 0.6187        | 3.23  | 19200 | 0.4256          | 0.5036 |
| 0.607         | 3.3   | 19600 | 0.4198          | 0.5027 |
| 0.6004        | 3.37  | 20000 | 0.4149          | 0.4906 |
| 0.6004        | 3.43  | 20400 | 0.4114          | 0.4902 |
| 0.6002        | 3.5   | 20800 | 0.4116          | 0.4967 |
| 0.5926        | 3.57  | 21200 | 0.4066          | 0.4843 |
| 0.5836        | 3.64  | 21600 | 0.3956          | 0.4791 |
| 0.588         | 3.7   | 22000 | 0.3941          | 0.4729 |
| 0.588         | 3.77  | 22400 | 0.3972          | 0.4799 |
| 0.5739        | 3.84  | 22800 | 0.4018          | 0.4790 |
| 0.5778        | 3.91  | 23200 | 0.3936          | 0.4750 |
| 0.5768        | 3.97  | 23600 | 0.3936          | 0.4751 |
| 0.5651        | 4.04  | 24000 | 0.3953          | 0.4706 |
| 0.5651        | 4.11  | 24400 | 0.3906          | 0.4659 |
| 0.5704        | 4.17  | 24800 | 0.3807          | 0.4557 |
| 0.5594        | 4.24  | 25200 | 0.3817          | 0.4610 |
| 0.5509        | 4.31  | 25600 | 0.3755          | 0.4553 |
| 0.5439        | 4.38  | 26000 | 0.3705          | 0.4471 |
| 0.5439        | 4.44  | 26400 | 0.3744          | 0.4487 |
| 0.5426        | 4.51  | 26800 | 0.3716          | 0.4483 |
| 0.5393        | 4.58  | 27200 | 0.3600          | 0.4356 |
| 0.5408        | 4.65  | 27600 | 0.3573          | 0.4307 |
| 0.5327        | 4.71  | 28000 | 0.3638          | 0.4382 |
| 0.5327        | 4.78  | 28400 | 0.3587          | 0.4316 |
| 0.5324        | 4.85  | 28800 | 0.3598          | 0.4290 |
| 0.5378        | 4.91  | 29200 | 0.3508          | 0.4243 |
| 0.5246        | 4.98  | 29600 | 0.3522          | 0.4260 |
| 0.5284        | 5.05  | 30000 | 0.3520          | 0.4268 |
| 0.5284        | 5.12  | 30400 | 0.3506          | 0.4224 |
| 0.5154        | 5.18  | 30800 | 0.3556          | 0.4223 |
| 0.5138        | 5.25  | 31200 | 0.3526          | 0.4276 |
| 0.51          | 5.32  | 31600 | 0.3440          | 0.4220 |
| 0.5065        | 5.39  | 32000 | 0.3367          | 0.4120 |
| 0.5065        | 5.45  | 32400 | 0.3406          | 0.4136 |
| 0.5087        | 5.52  | 32800 | 0.3370          | 0.4125 |
| 0.503         | 5.59  | 33200 | 0.3387          | 0.4134 |
| 0.5085        | 5.66  | 33600 | 0.3346          | 0.4068 |
| 0.5044        | 5.72  | 34000 | 0.3325          | 0.4057 |
| 0.5044        | 5.79  | 34400 | 0.3304          | 0.4026 |
| 0.4879        | 5.86  | 34800 | 0.3274          | 0.4002 |
| 0.4924        | 5.92  | 35200 | 0.3286          | 0.3980 |
| 0.4991        | 5.99  | 35600 | 0.3231          | 0.3952 |
| 0.487         | 6.06  | 36000 | 0.3324          | 0.4005 |
| 0.487         | 6.13  | 36400 | 0.3264          | 0.3952 |
| 0.4754        | 6.19  | 36800 | 0.3234          | 0.3905 |
| 0.4683        | 6.26  | 37200 | 0.3149          | 0.3840 |
| 0.4653        | 6.33  | 37600 | 0.3122          | 0.3824 |
| 0.4667        | 6.4   | 38000 | 0.3151          | 0.3855 |
| 0.4667        | 6.46  | 38400 | 0.3217          | 0.3859 |
| 0.4628        | 6.53  | 38800 | 0.3085          | 0.3831 |
| 0.4644        | 6.6   | 39200 | 0.3121          | 0.3791 |
| 0.4612        | 6.67  | 39600 | 0.3093          | 0.3790 |
| 0.4552        | 6.73  | 40000 | 0.3087          | 0.3749 |
| 0.4552        | 6.8   | 40400 | 0.3027          | 0.3679 |
| 0.4544        | 6.87  | 40800 | 0.3048          | 0.3672 |
| 0.4507        | 6.93  | 41200 | 0.2963          | 0.3614 |
| 0.4489        | 7.0   | 41600 | 0.3086          | 0.3718 |
| 0.4367        | 7.07  | 42000 | 0.3100          | 0.3754 |
| 0.4367        | 7.14  | 42400 | 0.3057          | 0.3701 |
| 0.4376        | 7.2   | 42800 | 0.2930          | 0.3614 |
| 0.428         | 7.27  | 43200 | 0.2907          | 0.3516 |
| 0.4241        | 7.34  | 43600 | 0.2916          | 0.3590 |
| 0.4312        | 7.41  | 44000 | 0.2904          | 0.3523 |
| 0.4312        | 7.47  | 44400 | 0.2908          | 0.3476 |
| 0.4292        | 7.54  | 44800 | 0.2858          | 0.3467 |
| 0.426         | 7.61  | 45200 | 0.2864          | 0.3484 |
| 0.4225        | 7.68  | 45600 | 0.2820          | 0.3441 |
| 0.422         | 7.74  | 46000 | 0.2834          | 0.3441 |
| 0.422         | 7.81  | 46400 | 0.2784          | 0.3420 |
| 0.4158        | 7.88  | 46800 | 0.2814          | 0.3390 |
| 0.4139        | 7.94  | 47200 | 0.2777          | 0.3384 |
| 0.4076        | 8.01  | 47600 | 0.2741          | 0.3381 |
| 0.3997        | 8.08  | 48000 | 0.2738          | 0.3320 |
| 0.3997        | 8.15  | 48400 | 0.2720          | 0.3303 |
| 0.4009        | 8.21  | 48800 | 0.2705          | 0.3357 |
| 0.3928        | 8.28  | 49200 | 0.2708          | 0.3265 |
| 0.3923        | 8.35  | 49600 | 0.2678          | 0.3283 |
| 0.3897        | 8.42  | 50000 | 0.2649          | 0.3241 |
| 0.3897        | 8.48  | 50400 | 0.2640          | 0.3218 |
| 0.3879        | 8.55  | 50800 | 0.2616          | 0.3197 |
| 0.3805        | 8.62  | 51200 | 0.2599          | 0.3170 |
| 0.3874        | 8.69  | 51600 | 0.2592          | 0.3168 |
| 0.3799        | 8.75  | 52000 | 0.2589          | 0.3157 |
| 0.3799        | 8.82  | 52400 | 0.2566          | 0.3137 |
| 0.3834        | 8.89  | 52800 | 0.2552          | 0.3141 |
| 0.3811        | 8.95  | 53200 | 0.2523          | 0.3108 |
| 0.3821        | 9.02  | 53600 | 0.2539          | 0.3112 |
| 0.3636        | 9.09  | 54000 | 0.2529          | 0.3070 |
| 0.3636        | 9.16  | 54400 | 0.2500          | 0.3078 |
| 0.3706        | 9.22  | 54800 | 0.2510          | 0.3067 |
| 0.367         | 9.29  | 55200 | 0.2497          | 0.3069 |
| 0.3618        | 9.36  | 55600 | 0.2493          | 0.3043 |
| 0.3624        | 9.43  | 56000 | 0.2491          | 0.3040 |
| 0.3624        | 9.49  | 56400 | 0.2466          | 0.3016 |
| 0.3557        | 9.56  | 56800 | 0.2460          | 0.3014 |
| 0.3536        | 9.63  | 57200 | 0.2470          | 0.2997 |
| 0.3584        | 9.7   | 57600 | 0.2441          | 0.2989 |
| 0.3563        | 9.76  | 58000 | 0.2442          | 0.2970 |
| 0.3563        | 9.83  | 58400 | 0.2436          | 0.2966 |
| 0.3492        | 9.9   | 58800 | 0.2431          | 0.2967 |
| 0.3483        | 9.96  | 59200 | 0.2428          | 0.2960 |


### Framework versions

- Transformers 4.17.0.dev0
- Pytorch 1.10.2+cu102
- Datasets 1.18.3
- Tokenizers 0.11.0
