---
language:
- tr
tags:
- zero-shot-classification
- nli
- pytorch
pipeline_tag: zero-shot-classification
license: mit
datasets:
- nli_tr
metrics:
- accuracy
widget:
- text: "Dolar yükselmeye devam ediyor."
  candidate_labels: "ekonomi, siyaset, spor"
- text: "Senaryo çok saçmaydı, beğendim diyemem."
  candidate_labels: "olumlu, olumsuz"
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# bert-base-multilingual-cased_allnli_tr

This model is a fine-tuned version of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6144
- Accuracy: 0.7662

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 32
- eval_batch_size: 32
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Accuracy |
|:-------------:|:-----:|:-----:|:---------------:|:--------:|
| 0.8623        | 0.03  | 1000  | 0.9076          | 0.5917   |
| 0.7528        | 0.07  | 2000  | 0.8587          | 0.6119   |
| 0.7074        | 0.1   | 3000  | 0.7867          | 0.6647   |
| 0.6949        | 0.14  | 4000  | 0.7474          | 0.6772   |
| 0.6681        | 0.17  | 5000  | 0.7661          | 0.6814   |
| 0.6597        | 0.2   | 6000  | 0.7264          | 0.6943   |
| 0.6495        | 0.24  | 7000  | 0.7841          | 0.6781   |
| 0.6323        | 0.27  | 8000  | 0.7256          | 0.6952   |
| 0.6308        | 0.31  | 9000  | 0.7319          | 0.6958   |
| 0.6254        | 0.34  | 10000 | 0.7054          | 0.7004   |
| 0.6233        | 0.37  | 11000 | 0.7069          | 0.7085   |
| 0.6165        | 0.41  | 12000 | 0.6880          | 0.7181   |
| 0.6033        | 0.44  | 13000 | 0.6844          | 0.7197   |
| 0.6014        | 0.48  | 14000 | 0.6753          | 0.7129   |
| 0.5947        | 0.51  | 15000 | 0.7000          | 0.7039   |
| 0.5965        | 0.54  | 16000 | 0.6708          | 0.7263   |
| 0.5979        | 0.58  | 17000 | 0.6562          | 0.7285   |
| 0.5787        | 0.61  | 18000 | 0.6554          | 0.7297   |
| 0.58          | 0.65  | 19000 | 0.6544          | 0.7315   |
| 0.574         | 0.68  | 20000 | 0.6549          | 0.7339   |
| 0.5751        | 0.71  | 21000 | 0.6545          | 0.7289   |
| 0.5659        | 0.75  | 22000 | 0.6467          | 0.7371   |
| 0.5732        | 0.78  | 23000 | 0.6448          | 0.7362   |
| 0.5637        | 0.82  | 24000 | 0.6520          | 0.7355   |
| 0.5648        | 0.85  | 25000 | 0.6412          | 0.7345   |
| 0.5622        | 0.88  | 26000 | 0.6350          | 0.7358   |
| 0.5579        | 0.92  | 27000 | 0.6347          | 0.7393   |
| 0.5518        | 0.95  | 28000 | 0.6417          | 0.7392   |
| 0.5547        | 0.99  | 29000 | 0.6321          | 0.7437   |
| 0.524         | 1.02  | 30000 | 0.6430          | 0.7412   |
| 0.4982        | 1.05  | 31000 | 0.6253          | 0.7458   |
| 0.5002        | 1.09  | 32000 | 0.6316          | 0.7418   |
| 0.4993        | 1.12  | 33000 | 0.6197          | 0.7487   |
| 0.4963        | 1.15  | 34000 | 0.6307          | 0.7462   |
| 0.504         | 1.19  | 35000 | 0.6272          | 0.7480   |
| 0.4922        | 1.22  | 36000 | 0.6410          | 0.7433   |
| 0.5016        | 1.26  | 37000 | 0.6295          | 0.7461   |
| 0.4957        | 1.29  | 38000 | 0.6183          | 0.7506   |
| 0.4883        | 1.32  | 39000 | 0.6261          | 0.7502   |
| 0.4985        | 1.36  | 40000 | 0.6315          | 0.7496   |
| 0.4885        | 1.39  | 41000 | 0.6189          | 0.7529   |
| 0.4909        | 1.43  | 42000 | 0.6189          | 0.7473   |
| 0.4894        | 1.46  | 43000 | 0.6314          | 0.7433   |
| 0.4912        | 1.49  | 44000 | 0.6184          | 0.7446   |
| 0.4851        | 1.53  | 45000 | 0.6258          | 0.7461   |
| 0.4879        | 1.56  | 46000 | 0.6286          | 0.7480   |
| 0.4907        | 1.6   | 47000 | 0.6196          | 0.7512   |
| 0.4884        | 1.63  | 48000 | 0.6157          | 0.7526   |
| 0.4755        | 1.66  | 49000 | 0.6056          | 0.7591   |
| 0.4811        | 1.7   | 50000 | 0.5977          | 0.7582   |
| 0.4787        | 1.73  | 51000 | 0.5915          | 0.7621   |
| 0.4779        | 1.77  | 52000 | 0.6014          | 0.7583   |
| 0.4767        | 1.8   | 53000 | 0.6041          | 0.7623   |
| 0.4737        | 1.83  | 54000 | 0.6093          | 0.7563   |
| 0.4836        | 1.87  | 55000 | 0.6001          | 0.7568   |
| 0.4765        | 1.9   | 56000 | 0.6109          | 0.7601   |
| 0.4776        | 1.94  | 57000 | 0.6046          | 0.7599   |
| 0.4769        | 1.97  | 58000 | 0.5970          | 0.7568   |
| 0.4654        | 2.0   | 59000 | 0.6147          | 0.7614   |
| 0.4144        | 2.04  | 60000 | 0.6439          | 0.7566   |
| 0.4101        | 2.07  | 61000 | 0.6373          | 0.7527   |
| 0.4192        | 2.11  | 62000 | 0.6136          | 0.7575   |
| 0.4128        | 2.14  | 63000 | 0.6283          | 0.7560   |
| 0.4204        | 2.17  | 64000 | 0.6187          | 0.7625   |
| 0.4114        | 2.21  | 65000 | 0.6127          | 0.7621   |
| 0.4097        | 2.24  | 66000 | 0.6188          | 0.7626   |
| 0.4129        | 2.28  | 67000 | 0.6156          | 0.7639   |
| 0.4085        | 2.31  | 68000 | 0.6232          | 0.7616   |
| 0.4074        | 2.34  | 69000 | 0.6240          | 0.7605   |
| 0.409         | 2.38  | 70000 | 0.6153          | 0.7591   |
| 0.4046        | 2.41  | 71000 | 0.6375          | 0.7587   |
| 0.4117        | 2.45  | 72000 | 0.6145          | 0.7629   |
| 0.4002        | 2.48  | 73000 | 0.6279          | 0.7610   |
| 0.4042        | 2.51  | 74000 | 0.6176          | 0.7646   |
| 0.4055        | 2.55  | 75000 | 0.6277          | 0.7643   |
| 0.4021        | 2.58  | 76000 | 0.6196          | 0.7642   |
| 0.4081        | 2.62  | 77000 | 0.6127          | 0.7659   |
| 0.408         | 2.65  | 78000 | 0.6237          | 0.7638   |
| 0.3997        | 2.68  | 79000 | 0.6190          | 0.7636   |
| 0.4093        | 2.72  | 80000 | 0.6152          | 0.7648   |
| 0.4095        | 2.75  | 81000 | 0.6155          | 0.7627   |
| 0.4088        | 2.79  | 82000 | 0.6130          | 0.7641   |
| 0.4063        | 2.82  | 83000 | 0.6072          | 0.7646   |
| 0.3978        | 2.85  | 84000 | 0.6128          | 0.7662   |
| 0.4034        | 2.89  | 85000 | 0.6157          | 0.7627   |
| 0.4044        | 2.92  | 86000 | 0.6127          | 0.7661   |
| 0.403         | 2.96  | 87000 | 0.6126          | 0.7664   |
| 0.4033        | 2.99  | 88000 | 0.6144          | 0.7662   |


### Framework versions

- Transformers 4.12.3
- Pytorch 1.10.0+cu102
- Datasets 1.15.1
- Tokenizers 0.10.3
