---
language: es
tags:
- Long documents
- LongFormer
- QA
- Q&A
datasets:
- BSC-TeMU/SQAC

---

# Spanish Longformer fine-tuned on **SQAC** for Spanish **QA** ğŸ“–â“
[longformer-base-4096-spanish](https://huggingface.co/mrm8488/longformer-base-4096-spanish) fine-tuned on [SQAC](https://huggingface.co/datasets/BSC-TeMU/SQAC) for **Q&A** downstream task.

## Details of the model ğŸ§ 
[longformer-base-4096-spanish](https://huggingface.co/mrm8488/longformer-base-4096-spanish) is a BERT-like model started from the RoBERTa checkpoint (**BERTIN** in this case) and pre-trained for *MLM* on long documents (from BETO's `all_wikis`). It supports sequences of length up to **4,096**! 

## Details of the dataset ğŸ“š 

This dataset contains 6,247 contexts and 18,817 questions with their answers, 1 to 5 for each fragment.
The sources of the contexts are:
* Encyclopedic articles from [Wikipedia in Spanish](https://es.wikipedia.org/), used under [CC-by-sa licence](https://creativecommons.org/licenses/by-sa/3.0/legalcode). 
* News from [Wikinews in Spanish](https://es.wikinews.org/), used under [CC-by licence](https://creativecommons.org/licenses/by/2.5/). 
* Text from the Spanish corpus [AnCora](http://clic.ub.edu/corpus/en), which is a mix from diferent newswire and literature sources, used under [CC-by licence](https://creativecommons.org/licenses/by/4.0/legalcode). 
This dataset can be used to build extractive-QA.

## Evaluation Metrics ğŸ“ˆ

TBA

## Fast Usage with HF `pipeline` ğŸ§ª
```py
from transformers import pipeline
qa_pipe = pipeline("question-answering", model='mrm8488/longformer-base-4096-spanish-finetuned-squad')
context = '''
Hace aproximadamente un aÃ±o, Hugging Face, una startup de procesamiento de lenguaje natural con sede en Brooklyn, Nueva York, lanzÃ³ BigScience, un proyecto internacional con mÃ¡s de 900 investigadores que estÃ¡ diseÃ±ado para comprender mejor y mejorar la calidad de los grandes modelos de lenguaje natural. Los modelos de lenguaje grande (LLM), algoritmos que pueden reconocer, predecir y generar lenguaje sobre la base de conjuntos de datos basados â€‹â€‹en texto, han captado la atenciÃ³n de empresarios y entusiastas de la tecnologÃ­a por igual. Pero el costoso hardware requerido para desarrollar LLM los ha mantenido en gran medida fuera del alcance de los investigadores sin los recursos de compaÃ±Ã­as como OpenAI y DeepMind detrÃ¡s de ellos.

InspirÃ¡ndose en organizaciones como la OrganizaciÃ³n Europea para la InvestigaciÃ³n Nuclear (tambiÃ©n conocida como CERN) y el Gran Colisionador de Hadrones, el objetivo de BigScience es crear LLM y grandes conjuntos de datos de texto que eventualmente serÃ¡n de cÃ³digo abierto para la IA mÃ¡s amplia. comunidad. Los modelos serÃ¡n entrenados en la supercomputadora Jean Zay ubicada cerca de ParÃ­s, Francia, que se encuentra entre las mÃ¡quinas mÃ¡s poderosas del mundo.
'''
question = "Â¿CuÃ¡l es el objetivo de BigScience?"

qa_pipe({'context':context, 'question': question})
# It outpus
```
```js
{'answer': 'comprender mejor y mejorar la calidad de los grandes modelos de lenguaje natural.',
 'end': 305,
 'score': 0.9999799728393555,
 'start': 224}
 ```
 
 > Created by [Manuel Romero/@mrm8488](https://twitter.com/mrm8488) with the support of [Narrativa](https://www.narrativa.com/)

> Made with <span style="color: #e25555;">&hearts;</span> in Spain
