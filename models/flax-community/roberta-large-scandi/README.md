# Scandinavian RoBERTa
A RoBERTa-large model trained from scratch on Swedish, Danish, Norwegian,
Icelandic (and maybe also Faroese if we can get enough data) using 
[Flax](https://github.com/google/flax), including training scripts.

This is part of the
[Flax/Jax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104),
organised by [HuggingFace](https://huggingface.co/) and TPU usage sponsored by Google.

## Team members
- saattrupdan | Dan Saattrup Nielsen | [saattrupdan@gmail.com](mailto:saattrupdan@gmail.com)
- MortenKP | Morten Kloster Pedersen
- Maltehb | Malte Højmark-Bertelsen
- pere | Per
- versae | Javier de la Rosa
- Juunge | Kasper Junge
- Gabriel | Gabriel Borg
- ?xxzyx? | Xander Nordentord
- ?grofte? | Morten Grøftehauge


## Useful links
- [Community Week timeline](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104#summary-timeline-calendar-6)
- [Community Week README](https://github.com/huggingface/transformers/blob/master/examples/research_projects/jax-projects/README.md)
- [Community Week thread](https://discuss.huggingface.co/t/scandinavian-roberta/7533)
- [Masked Language Modelling example scripts](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling)
- [HuggingFace Model Repository](https://huggingface.co/saattrupdan/scandinavian-roberta)
