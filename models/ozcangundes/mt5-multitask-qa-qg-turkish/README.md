---
language: tr
datasets:
- TQUAD
tags: 
- question-answering
- question-generation
- multitask-model

license: apache-2.0
---

# mT5-small based Turkish Multitask (Answer Extraction, Question Generation and Question Answering) System

[Google's Multilingual T5-small](https://github.com/google-research/multilingual-t5) is fine-tuned on [Turkish Question Answering dataset](https://github.com/okanvk/Turkish-Reading-Comprehension-Question-Answering-Dataset) for three downstream task **Answer extraction, Question Generation and Question Answering** served in this single model. mT5 model was also trained for multiple text2text NLP tasks. 

All data processing, training and pipeline codes can be found on my [Github](https://github.com/ozcangundes/multitask-question-generation). I will share the training details in the repo as soon as possible. 

mT5 small model has 300 million parameters and model size is about 1.2GB. Therefore, it takes significant amount of time to fine tune it. 

8 epoch and 1e-4 learning rate with 0 warmup steps was applied during training. These hparams and the others can be fine-tuned for much more better results.

## Requirements â—â—â—
```
!pip install transformers==4.4.2
!pip install sentencepiece==0.1.95
!git clone https://github.com/ozcangundes/multitask-question-generation.git
%cd multitask-question-generation/
```

## Usage ğŸš€ğŸš€
```
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("ozcangundes/mt5-multitask-qa-qg-turkish") 
model = AutoModelForSeq2SeqLM.from_pretrained("ozcangundes/mt5-multitask-qa-qg-turkish")

from pipelines import pipeline #pipelines.py script in the cloned repo
multimodel = pipeline("multitask-qa-qg",tokenizer=tokenizer,model=model)

#sample text
text="Ã–zcan GÃ¼ndeÅŸ, 1993 yÄ±lÄ± Tarsus doÄŸumludur. Orta DoÄŸu Teknik Ãœniversitesi \\\\
EndÃ¼stri MÃ¼hendisliÄŸi bÃ¶lÃ¼mÃ¼nde 2011 2016 yÄ±llarÄ± arasÄ±nda lisans eÄŸitimi gÃ¶rmÃ¼ÅŸtÃ¼r. \\\\
YÃ¼ksek lisansÄ±nÄ± ise 2020 AralÄ±k ayÄ±nda, 4.00 genel not ortalamasÄ± ile \\\\
BoÄŸaziÃ§i Ãœniversitesi, YÃ¶netim BiliÅŸim Sistemleri bÃ¶lÃ¼mÃ¼nde tamamlamÄ±ÅŸtÄ±r.\\\\
Futbolla yakÄ±ndan ilgilenmekle birlikte, Galatasaray kulÃ¼bÃ¼ taraftarÄ±dÄ±r."
```

## Example - Both Question Generation and Question Answering ğŸ’¬ğŸ’¬
```
multimodel(text)

#output
=> [{'answer': 'Tarsus', 'question': 'Ã–zcan GÃ¼ndeÅŸ nerede doÄŸmuÅŸtur?'},
 {'answer': '1993', 'question': 'Ã–zcan GÃ¼ndeÅŸ kaÃ§ yÄ±lÄ±nda doÄŸmuÅŸtur?'},
 {'answer': '2011 2016',
  'question': 'Ã–zcan GÃ¼ndeÅŸ lisans eÄŸitimini hangi yÄ±llar arasÄ±nda tamamlamÄ±ÅŸtÄ±r?'},
 {'answer': 'BoÄŸaziÃ§i Ãœniversitesi, YÃ¶netim BiliÅŸim Sistemleri',
  'question': 'Ã–zcan GÃ¼ndeÅŸ yÃ¼ksek lisansÄ±nÄ± hangi bÃ¶lÃ¼mde tamamlamÄ±ÅŸtÄ±r?'},
 {'answer': 'Galatasaray kulÃ¼bÃ¼',
  'question': 'Ã–zcan GÃ¼ndeÅŸ futbolla yakÄ±ndan ilgilenmekle birlikte hangi kulÃ¼bÃ¼ taraftarÄ±dÄ±r?'}]
```
From this text, 5 questions are generated and they are answered by the model. 

## Example - Question Answering ğŸ’­ğŸ’­

Both text and also, related question should be passed into pipeline.
```
multimodel({"context":text,"question":"Ã–zcan hangi takÄ±mÄ± tutmaktadÄ±r?"})

#output
=> Galatasaray

multimodel({"context":text,"question":"Ã–zcan, yÃ¼ksek lisanstan ne zaman mezun oldu?"})

#output
=> 2020 AralÄ±k ayÄ±nda


multimodel({"context":text,"question":"Ã–zcan'Ä±n yÃ¼ksek lisans bitirme notu kaÃ§tÄ±r?"})

#output
=> 4.00 

#Sorry for being cocky ğŸ˜ğŸ˜
```

## ACKNOWLEDGEMENT

This work is inspired from [Suraj Patil's great repo](https://github.com/patil-suraj/question_generation). I would like to thank him for the clean codes and also,[Okan Ã‡iftÃ§i](https://github.com/okanvk) for the Turkish dataset ğŸ™

